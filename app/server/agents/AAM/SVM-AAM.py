"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JJnAQWA5B2ZDNZvrrYKvK5H3hK4LpLa5
"""

import pandas as pd
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,ReLU
from keras.losses import categorical_crossentropy
from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint
import keras.backend as K
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.svm import LinearSVC
from joblib import dump,load
import numpy as np
from PIL import Image as im
from app.server.agents.ImageProcessor import ImageProcessor
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf
import pickle


def evaluate(model, X, Y):
    predicted_Y = model.predict(X)
    accuracy = accuracy_score(Y, predicted_Y)
    return accuracy

def matrix(svm_model,feat_valid,feat_test):


    validation_accuracy = evaluate(svm_model,feat_valid,y_valid)
    print("  - validation accuracy = {0:.1f}%".format(validation_accuracy*100))
    valid_labels = svm_model.predict(feat_valid)
    print(classification_report(y_valid, valid_labels))
    mat = confusion_matrix(y_valid, valid_labels)
    print(mat)


    test_accuracy = evaluate(svm_model, feat_test, y_test)
    print("  - test accuracy = {0:.1f}%".format(test_accuracy*100))
    test_labels = svm_model.predict(feat_test)
    print(classification_report(y_test, test_labels))
    mat = confusion_matrix(y_test, test_labels)
    print(mat)

def toPng():
    # create a numpy array from scratch
    # using arange function.
    # 1024x720 = 737280 is the amount
    # of pixels.
    # np.uint8 is a data type containing
    # numbers ranging from 0 to 255
    # and no non-negative integers
    array = np.arange(0, 256*256, 1, np.uint8)

    # check type of array
    print(type(array))

    # our array will be of width
    # 737280 pixels That means it
    # will be a long dark line
    print(array.shape)

    # Reshape the array into a
    # familiar resoluition
    array = np.reshape(array, (256, 256))

    # show the shape of the array
    print(array.shape)

    # show the array
    print(array)

    # creating image object of
    # above array
    data = im.fromarray(array)

    # saving the final output
    # as a PNG file
    data.save('gfg_dummy_pic.png')

def processData(path):
    #read data
    df=pd.read_csv(path,sep=' ')
    #shuffle data
    df = df.sample(frac = 1)
    #split data into training,validation and test data(70,10,20)
    df.isnull().sum()
    print(df.describe)
    df['pixels'] = df['pixels'].apply(lambda im: np.fromstring(im, sep=' '))
    x_train = np.vstack(df['pixels'][0:int(len(df)*0.7)].values)
    y_train = np.array(df["emotion"][0:int(len(df)*0.7)])

    x_valid = np.vstack(df["pixels"][int(len(df)*0.7):int(len(df)*0.8)].values)
    y_valid = np.array(df["emotion"][int(len(df)*0.7):int(len(df)*0.8)])

    x_test = np.vstack(df["pixels"][int(len(df)*0.8):len(df)].values)
    y_test = np.array(df["emotion"][int(len(df)*0.8):len(df)])

    #normalize x
    # x_train=np.array(x_train)/255.0
    # x_valid=np.array(x_valid)/255.0
    # x_test=np.array(x_test)/255.0

    #reshape x into 2D
    N = len(x_train)
    x_train = x_train.reshape(N, 256, 256, 1)
    N = len(x_valid)
    x_valid = x_valid.reshape(N, 256, 256, 1)
    N = len(x_test)
    x_test = x_test.reshape(N, 256, 256, 1)

    #reshape y into 2D
    num_class=len(set(y_test))
    Y_train = (np.arange(num_class) == y_train[:, None]).astype(np.float32)
    Y_valid = (np.arange(num_class) == y_valid[:, None]).astype(np.float32)
    Y_test = (np.arange(num_class) == y_test[:, None]).astype(np.float32)

    return x_train,y_train,x_valid,y_valid,x_test,y_test,Y_train,Y_valid,Y_test

def anotherModel():
    model = Sequential()
    input_shape = (256,256,1)
    model.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))
    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))
    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))
    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    ############
#     model.add(Flatten())
#     model.add(Dense(256, name="dense_one"))
#     model.add(BatchNormalization())
#     model.add(Activation('relu'))
#     model.add(Dropout(0.2))
    ###########

    model.add(Flatten())
    model.add(Dense(128, name="dense_one"))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.5))

    model.add(Dense(7))
    model.add(Activation(tf.nn.softmax))

    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')
    K.set_value(model.optimizer.lr,1e-3)
    # UNCOMMENT THIS TO VIEW THE ARCHITECTURE
    model.summary()

    return model

def makeModel():
    model=Sequential()
    model.add(Conv2D(input_shape=(256,256,1),filters=96,kernel_size=(11,11),padding='same',name='conv1'))
    model.add(ReLU(name='relu1'))
    model.add(Conv2D(filters=96,kernel_size=11,padding='same'))
    model.add(MaxPooling2D(pool_size=(3,3),strides=2,name='pool'))
    model.add(BatchNormalization())
    #model.add(Dropout(0.5))
    #model.add(loca)

    model.add(Conv2D(filters=96,kernel_size=11,padding='same',activation='relu',name='conv2'))
    model.add(ReLU(name='relu2'))
    model.add(Conv2D(filters=96,kernel_size=11,padding='same'))
    model.add(MaxPooling2D(pool_size=(3,3),strides=2,name='pool2'))
    model.add(BatchNormalization())
    #model.add(Dropout(0.5))
    #model.add(lrn(beta=5,name='lrn2'))

    model.add(Conv2D(filters=96,kernel_size=11,activation='relu',padding='same',name='conv3'))
    model.add(ReLU(name='relu3'))
    model.add(Conv2D(filters=96,kernel_size=11,padding='same'))

    model.add(Conv2D(filters=96,kernel_size=11,activation='relu',padding='same',name='conv4'))
    model.add(ReLU(name='relu4'))
    model.add(Conv2D(filters=96,kernel_size=11,padding='same'))

    model.add(Conv2D(filters=96,kernel_size=11,activation='relu',padding='same',name='conv5'))
    model.add(ReLU(name='relu5'))
    model.add(Conv2D(filters=96,kernel_size=11,padding='same'))
    model.add(MaxPooling2D(pool_size=(3,3),strides=2,name='pool5'))

    #TBC
    model.add(Flatten())
    model.add(Dense(128, name="dense_one"))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    model.add(Dense(7))
    model.add(Activation('softmax'))
    model.summary()

    model.compile(
            loss=categorical_crossentropy,
            optimizer='adam',
            # optimizer=Adagrad(),
            # optimizer=RMSprop(learning_rate=0.001, rho=0.9),
            metrics=['accuracy']
            )

    #model.tensorflow_backend.clear_session() # destroys the current graph and builds a new one
    K.set_value(model.optimizer.lr,1e-3) # set the learning rate
    return model

def trainDCNN():

    path_model= 'anotherModel_filter.h5'  # save model at this location after each epoch

    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.90, patience=3, verbose=1)

    early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')

    h=model.fit(x=x_train,
                y=Y_train,
              batch_size=64,
              epochs=2,
              verbose=1,
              callbacks=[early_stopper, lr_reducer,ModelCheckpoint(filepath=path_model,save_weights_only=True)],
              validation_data=(x_valid,Y_valid),
              shuffle=True,
              steps_per_epoch=8
              #steps_per_epoch=1
              )

def fitEvaluateSVC():
    model_feat = Model(inputs=model.input, outputs=model.get_layer('dense_one').output)

    print(model.input, model.get_layer('dense_one').output)

    feat_train = model_feat.predict(x_train)
    print(feat_train.shape)

    feat_valid = model_feat.predict(x_valid)
    print(feat_valid.shape)

    feat_test = model_feat.predict(x_test)
    print(feat_test.shape)

    svm_model_1 = LinearSVC(C=100.0,max_iter=100000)
    svm_model_1.fit(feat_train, y_train)
    print('fitting done !!!')
    dump(svm_model_1, '../cache_results/svm_model.joblib')

    matrix(svm_model_1,feat_valid,feat_test)

    return svm_model_1

def loadModelTensor(path):
    model.load_weights(path)
    model.summary()

def loadModelSK(path):
    svmModel=load('../cache_results/svm_model.joblib')
    return svmModel

def Predict(featureModel,classifyModel,x):
    featureModel = Model(inputs=featureModel.input,outputs=featureModel.get_layer('dense_one').output)
    x=featureModel.predict(x)
    predictedY=classifyModel.predict(x)
    print(predictedY)


x_train,y_train,x_valid,y_valid,x_test,y_test,Y_train,Y_valid,Y_test=processData("C:\\Users\\Catalin\\Desktop\\facultate\\licenta\\DB\\db.csv")

model=anotherModel()

# #model=makeModel()
loadModelTensor("C:\\Users\\Catalin\\Desktop\\facultate\\Semestru_5\\MIRPR\\laborator\\aplicatie-git\\New folder\\mirpr-calculafectiv\\app\\server\\agents\\cache_results\\anotherModel_filter.h5")#TBA
# trainDCNN()
# svmModel=fitEvaluateSVC()
svmModel=loadModelSK("path")#TBA



processor = ImageProcessor()
listPixels=processor.generate("C:\\Users\\Catalin\\Desktop\\facultate\\Semestru_5\\MIRPR\\laborator\\aplicatie-git\\New folder\\mirpr-calculafectiv\\app\\server\\agents\\test_images\\putin.png", True)
# listPixels=processor.generate("C:\\Users\\Bubu\\mirpr-2020-21\\app\\server\\agents\\S032_004_00000014.png", True)


Predict(model, svmModel, listPixels)
# Predict(model, svmModel, x_train[0])
'''
plt.imshow(mpimg.imread("C:\\Users\\Catalin\\Desktop\\facultate\\Semestru_5\\MIRPR\\laborator\\aplicatie-git\\New folder\\mirpr-calculafectiv\\app\\server\\agents\\test_images\\putin.png"))
plt.show()
print("end")
'''

# for pixels in listPixels:
#     Predict(model,svmModel,pixels)
